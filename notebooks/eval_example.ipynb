{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../src/pgm')\n",
    "sys.path.append('../morphomnist')\n",
    "from typing import Dict, IO, Optional, Tuple, List\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pgm.train_pgm import setup_dataloaders, preprocess\n",
    "from pgm.flow_pgm import MorphoMNISTPGM\n",
    "\n",
    "class Hparams:\n",
    "    def update(self, dict):\n",
    "        for k, v in dict.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code needed for running evaluation should already be here:\n",
    "\n",
    "(i) https://github.com/biomedia-mira/causal-gen/blob/main/src/pgm/train_cf.py\n",
    "\n",
    "(ii) https://huggingface.co/spaces/mira-causality/counterfactuals/blob/main/app_utils.py\n",
    "\n",
    "I whipped up a more concise eval example below, hope it helps :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MorphoMNIST\n",
    "1. Load parent predictors - classifier/regressor for each parent in $\\mathbf{pa}_\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_path = '../checkpoints/sup_aux/checkpoint.pt'\n",
    "print(f'\\nLoading predictor checkpoint: {predictor_path}')\n",
    "predictor_checkpoint = torch.load(predictor_path)\n",
    "predictor_args = Hparams()\n",
    "predictor_args.update(predictor_checkpoint['hparams'])\n",
    "assert predictor_args.dataset == 'morphomnist'\n",
    "predictor = MorphoMNISTPGM(predictor_args).cuda()\n",
    "predictor.load_state_dict(predictor_checkpoint['ema_model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load PGM - Flow-based causal mechanisms for each node in the causal graph (except for $\\mathbf{x}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgm_path = '../checkpoints/sup_pgm/checkpoint.pt'\n",
    "print(f'\\nLoading PGM checkpoint: {pgm_path}')\n",
    "pgm_checkpoint = torch.load(pgm_path)\n",
    "pgm_args = Hparams()\n",
    "pgm_args.update(pgm_checkpoint['hparams'])\n",
    "assert pgm_args.dataset == 'morphomnist'\n",
    "pgm = MorphoMNISTPGM(pgm_args).cuda()\n",
    "pgm.load_state_dict(pgm_checkpoint['ema_model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load HVAE - causal mechanism for the image $\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vae(vae_path):\n",
    "    print(f'\\nLoading VAE checkpoint: {vae_path}')\n",
    "    vae_checkpoint = torch.load(vae_path)\n",
    "    vae_args = Hparams()\n",
    "    vae_args.update(vae_checkpoint['hparams'])\n",
    "    vae_args.data_dir = 'your dataset dir here'\n",
    "\n",
    "    # init model\n",
    "    assert vae_args.hps == 'morphomnist'\n",
    "    if not hasattr(vae_args, 'vae'):\n",
    "        vae_args.vae = 'simple'\n",
    "\n",
    "    if vae_args.vae == 'hierarchical':\n",
    "        from vae import HVAE\n",
    "        vae = HVAE(vae_args).cuda()\n",
    "    elif vae_args.vae == 'simple':\n",
    "        from simple_vae import VAE\n",
    "        vae = VAE(vae_args).cuda()\n",
    "    else:\n",
    "        NotImplementedError\n",
    "    vae.load_state_dict(vae_checkpoint['ema_model_state_dict'])\n",
    "    return vae, vae_args\n",
    "\n",
    "model_name = ''\n",
    "vae_path = '../checkpoints/t_i_d/'+model_name+'/checkpoint.pt'\n",
    "vae, vae_args = load_vae(vae_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Counterfactual Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from morphomnist.morpho import ImageMorphology\n",
    "# Refer to https://github.com/dccastro/Morpho-MNIST for details on Morpho-MNIST\n",
    "\n",
    "def get_intensity(x, threshold=0.5):\n",
    "    x = x.detach().cpu().numpy()[:, 0]\n",
    "    x_min, x_max = x.min(axis=(1, 2), keepdims=True), x.max(axis=(1, 2), keepdims=True)\n",
    "    mask = (x >= x_min + (x_max - x_min) * threshold)\n",
    "    return np.array([np.median(i[m]) for i, m in zip(x, mask)])\n",
    "\n",
    "def img_thickness(img, threshold, scale):\n",
    "    return ImageMorphology(np.asarray(img), threshold, scale).mean_thickness\n",
    "\n",
    "def unpack(args):\n",
    "    return img_thickness(*args)\n",
    "\n",
    "def get_thickness(x, threshold=0.5, scale=4, pool=None, chunksize=100):\n",
    "    imgs = x.detach().cpu().numpy()[:, 0]\n",
    "    args = ((img, threshold, scale) for img in imgs)\n",
    "    if pool is None:\n",
    "        gen = map(unpack, args)\n",
    "    else:\n",
    "        gen = pool.imap(unpack, args, chunksize=chunksize)\n",
    "    results = tqdm(gen, total=len(imgs), unit='img', ascii=True)\n",
    "    return list(results)\n",
    "\n",
    "def vae_preprocess(pa: Dict[str, Tensor], input_res: int = 32) -> Tensor:\n",
    "    # concatenate parents and expand to input resolution for vae input\n",
    "    pa = torch.cat([\n",
    "        pa[k] if len(pa[k].shape) > 1 else pa[k].unsqueeze(-1) for k in pa.keys()\n",
    "    ], dim=1)\n",
    "    return pa[...,None,None].repeat(1, 1, *(input_res,)*2)\n",
    "\n",
    "@torch.no_grad()\n",
    "def cf_epoch(\n",
    "    vae: nn.Module, \n",
    "    pgm: nn.Module, \n",
    "    predictor: nn.Module, \n",
    "    dataloaders: Dict[str, DataLoader],\n",
    "    do_pa: Optional[str] = None, \n",
    "    te_cf: bool = False\n",
    ") -> Tuple[Tensor, Tensor, Tensor]:\n",
    "    vae.eval()\n",
    "    pgm.eval()\n",
    "    predictor.eval()\n",
    "    dag_vars = list(pgm.variables.keys())\n",
    "    preds = {k: [] for k in dag_vars}\n",
    "    targets = {k: [] for k in dag_vars}\n",
    "    x_counterfactuals = []\n",
    "    train_set = copy.deepcopy(dataloaders['train'].dataset.samples)\n",
    "    loader = tqdm(enumerate(dataloaders['test']), total=len(\n",
    "        dataloaders['test']), mininterval=0.1)\n",
    "\n",
    "    for _, batch in loader:\n",
    "        bs = batch['x'].shape[0]\n",
    "        batch = preprocess(batch)\n",
    "        pa = {k: v for k, v in batch.items() if k != 'x'}\n",
    "        # randomly intervene on a single parent do(pa_k), pa_k ~ p(pa_k)\n",
    "        do = {}\n",
    "        if do_pa is not None:\n",
    "            idx = torch.randperm(train_set[do_pa].shape[0])\n",
    "            do[do_pa] = train_set[do_pa].clone()[idx][:bs]\n",
    "        else: # random interventions\n",
    "            while not do:\n",
    "                for k in dag_vars:\n",
    "                    if torch.rand(1) > 0.5:  # coin flip to intervene on pa_k\n",
    "                        idx = torch.randperm(train_set[k].shape[0])\n",
    "                        do[k] = train_set[k].clone()[idx][:bs]\n",
    "        do = preprocess(do)\n",
    "        # infer counterfactual parents\n",
    "        cf_pa = pgm.counterfactual(obs=pa, intervention=do, num_particles=1)\n",
    "        _pa = vae_preprocess({k: v.clone() for k, v in pa.items()})\n",
    "        _cf_pa = vae_preprocess({k: v.clone() for k, v in cf_pa.items()})\n",
    "        # abduct exogenous noise z\n",
    "        t_z = t_u = 0.1  # sampling temp\n",
    "        z = vae.abduct(batch['x'], parents=_pa, t=t_z)\n",
    "        if vae.cond_prior:\n",
    "            z = [z[i]['z'] for i in range(len(z))]\n",
    "        # forward vae with observed parents\n",
    "        rec_loc, rec_scale = vae.forward_latents(z, parents=_pa)\n",
    "        # abduct exogenous noise u\n",
    "        u = (batch['x'] - rec_loc) / rec_scale.clamp(min=1e-12)\n",
    "        if vae.cond_prior and te_cf:  # g(z*, pa*)\n",
    "            # infer counterfactual mediator z*\n",
    "            cf_z = vae.abduct(x=batch['x'], parents=_pa, cf_parents=_cf_pa, alpha=0.65)\n",
    "            cf_loc, cf_scale = vae.forward_latents(cf_z, parents=_cf_pa)\n",
    "        else:  # g(z, pa*)\n",
    "            cf_loc, cf_scale = vae.forward_latents(z, parents=_cf_pa)\n",
    "        cf_scale = cf_scale * t_u\n",
    "        cfs = {'x':  torch.clamp(cf_loc + cf_scale * u, min=-1, max=1)}\n",
    "        cfs.update(cf_pa)\n",
    "        x_counterfactuals.extend(cfs['x'])\n",
    "        # predict labels of inferred counterfactuals\n",
    "        preds_cf = predictor.predict(**cfs)\n",
    "        for k, v in preds_cf.items():\n",
    "            preds[k].extend(v)\n",
    "        # targets are the interventions and/or counterfactual parents\n",
    "        for k in targets.keys():\n",
    "            t_k = do[k].clone() if k in do.keys() else cfs[k].clone()\n",
    "            targets[k].extend(t_k)\n",
    "    for k, v in targets.items():\n",
    "        targets[k] = torch.stack(v).squeeze().cpu()\n",
    "        preds[k] = torch.stack(preds[k]).squeeze().cpu()\n",
    "    x_counterfactuals = torch.stack(x_counterfactuals).cpu()\n",
    "    return targets, preds, x_counterfactuals\n",
    "\n",
    "\n",
    "def eval_cf_loop(\n",
    "    vae: nn.Module,\n",
    "    pgm: nn.Module,\n",
    "    predictor: nn.Module,\n",
    "    dataloaders: Dict[str, DataLoader],\n",
    "    file: IO[str],\n",
    "    total_effect: bool = False,\n",
    "    seeds: List[int] = [0, 1, 2],\n",
    "):\n",
    "    for do_pa in ['thickness', 'intensity', 'digit', None]:  # \"None\" is for random interventions\n",
    "        acc_runs = []\n",
    "        mae_runs = {\n",
    "            'thickness': {'predicted': [], 'measured': []},\n",
    "            'intensity': {'predicted': [], 'measured': []}\n",
    "        }\n",
    "\n",
    "        for seed in seeds:\n",
    "            print(f'do({(do_pa if do_pa is not None else \"random\")}), seed {seed}:')\n",
    "            assert vae.cond_prior if total_effect else True\n",
    "            targets, preds, x_cfs = cf_epoch(vae, pgm, predictor, dataloaders, do_pa, total_effect)\n",
    "            acc = (targets['digit'].argmax(-1).numpy() == preds['digit'].argmax(-1).numpy()).mean()\n",
    "            print(f'predicted digit acc:', acc)\n",
    "            # evaluate inferred cfs using true causal mechanisms\n",
    "            measured = {}\n",
    "            measured['intensity'] = torch.tensor(get_intensity((x_cfs + 1.0) * 127.5))\n",
    "            with multiprocessing.Pool() as pool:\n",
    "                measured['thickness'] = torch.tensor(get_thickness((x_cfs + 1.0) * 127.5, pool=pool, chunksize=250))\n",
    "\n",
    "            mae = {'thickness': {}, 'intensity': {}}\n",
    "            for k in ['thickness', 'intensity']:\n",
    "                min_max = dataloaders['train'].dataset.min_max[k]\n",
    "                _min, _max = min_max[0], min_max[1]\n",
    "                preds_k = ((preds[k] + 1) / 2) * (_max - _min) + _min\n",
    "                targets_k = ((targets[k] + 1) / 2) * (_max - _min) + _min\n",
    "                mae[k]['predicted'] = (targets_k - preds_k).abs().mean().item()\n",
    "                mae[k]['measured'] = (targets_k - measured[k]).abs().mean().item()\n",
    "                print(f'predicted {k} mae:', mae[k]['predicted'])\n",
    "                print(f'measured {k} mae:', mae[k]['measured'])\n",
    "\n",
    "            acc_runs.append(acc)\n",
    "            for k in ['thickness', 'intensity']:\n",
    "                mae_runs[k]['predicted'].append(mae[k]['predicted'])\n",
    "                mae_runs[k]['measured'].append(mae[k]['measured'])\n",
    "\n",
    "            file.write(\n",
    "                f'\\ndo({(do_pa if do_pa is not None else \"random\")}) | digit acc: {acc}, ' +\n",
    "                f'thickness mae (predicted): {mae[\"thickness\"][\"predicted\"]}, ' +\n",
    "                f'thickness mae (measured): {mae[\"thickness\"][\"measured\"]}, ' +\n",
    "                f'intensity mae (predicted): {mae[\"intensity\"][\"predicted\"]}, ' +\n",
    "                f'intensity mae (measured): {mae[\"intensity\"][\"measured\"]} | seed {seed}'\n",
    "            )\n",
    "            file.flush()\n",
    "            gc.collect()\n",
    "\n",
    "        v = 'Total effect: '+ str(total_effect)\n",
    "        file.write(\n",
    "            f'\\n{(v if vae.cond_prior else \"\")}\\n' +\n",
    "            f'digit acc | mean: {np.array(acc_runs).mean()} - std: {np.array(acc_runs).std()}\\n' +\n",
    "            f'thickness mae (predicted) | mean: {np.array(mae_runs[\"thickness\"][\"predicted\"]).mean()} - std: {np.array(mae_runs[\"thickness\"][\"predicted\"]).std()}\\n' +\n",
    "            f'thickness mae (measured) | mean: {np.array(mae_runs[\"thickness\"][\"measured\"]).mean()} - std: {np.array(mae_runs[\"thickness\"][\"measured\"]).std()}\\n' +\n",
    "            f'intensity mae (predicted) | mean: {np.array(mae_runs[\"intensity\"][\"predicted\"]).mean()} - std: {np.array(mae_runs[\"intensity\"][\"predicted\"]).std()}\\n' +\n",
    "            f'intensity mae (measured) | mean: {np.array(mae_runs[\"intensity\"][\"measured\"]).mean()} - std: {np.array(mae_runs[\"intensity\"][\"measured\"]).std()}\\n'\n",
    "        )\n",
    "        file.flush()\n",
    "    return\n",
    "\n",
    "for model_name in [\n",
    "'add your model name(s) here'\n",
    "]:\n",
    "    file = open(f'./eval_{model_name}.txt', 'a')\n",
    "    vae_path = '../checkpoints/'+model_name+'/checkpoint.pt'\n",
    "    vae, vae_args = load_vae(vae_path)\n",
    "    assert pgm_args.dataset == 'morphomnist'\n",
    "    pgm_args.data_dir = 'your dataset dir here'\n",
    "    pgm_args.bs = 32\n",
    "    dataloaders = setup_dataloaders(pgm_args)\n",
    "    eval_cf_loop(vae, pgm, predictor, dataloaders, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UKBB/MIMIC\n",
    "1. Load parent predictors, PGM and HVAE similar to above\n",
    "2. Run counterfactual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import EMA\n",
    "from pgm.dscm import DSCM\n",
    "from pgm.layers import TraceStorage_ELBO\n",
    "\n",
    "# some compatibility stuff\n",
    "args = Hparams()\n",
    "args.beta = vae_args.beta\n",
    "args.parents_x = vae_args.parents_x\n",
    "args.input_res = vae_args.input_res\n",
    "args.grad_clip = vae_args.grad_clip\n",
    "args.grad_skip = vae_args.grad_skip\n",
    "args.elbo_constraint = 1.8412 # train set -elbo\n",
    "args.wd = vae_args.wd\n",
    "args.betas = vae_args.betas\n",
    "args.lmbda_init = 0\n",
    "args.damping = 100\n",
    "\n",
    "# init model\n",
    "if not hasattr(vae_args, 'dataset'):\n",
    "    args.dataset = 'ukbb'\n",
    "\n",
    "# Option 1: use base model (before counterfactual training)\n",
    "dscm = DSCM(args, pgm, predictor, vae)\n",
    "dscm.cuda();\n",
    "\n",
    "# Option 2: use counterfactual fine-tuned model\n",
    "dscm_path = os.path.join('../checkpoints', 'yourmodel', 'checkpoint.pt')\n",
    "print(f'\\nLoading DSCM checkpoint: {dscm_path}')\n",
    "dscm_checkpoint = torch.load(dscm_path)\n",
    "dscm_args = Hparams()\n",
    "dscm_args.update(dscm_checkpoint['hparams'])\n",
    "dscm_args.dataset = 'ukbb'\n",
    "dscm = DSCM(dscm_args, pgm, predictor, vae).cuda()\n",
    "dscm.load_state_dict(dscm_checkpoint['ema_model_state_dict'])\n",
    "dscm.cuda();\n",
    "\n",
    "# setup dataloaders\n",
    "pgm_args.bs = 8\n",
    "pgm_args.concat_pa = False\n",
    "dataloaders = setup_dataloaders(pgm_args)\n",
    "ema = EMA(dscm, beta=0.999)\n",
    "\n",
    "# set some defaults\n",
    "args.do_pa = None\n",
    "args.plot_freq = 1\n",
    "args.save_dir = './'\n",
    "args.step = 0\n",
    "args.imgs_plot = 10\n",
    "args.alpha = 1\n",
    "args.cf_particles = 1\n",
    "args.cond_prior = vae_args.cond_prior\n",
    "elbo_fn = TraceStorage_ELBO(num_particles=1)\n",
    "\n",
    "# run counterfactual evaluation\n",
    "with torch.no_grad():\n",
    "    copy_do_pa = copy.deepcopy(args.do_pa)\n",
    "    for pa_k in list(dscm.pgm.variables.keys()) + [None]:\n",
    "        args.do_pa = pa_k\n",
    "        valid_stats, metrics = cf_epoch(\n",
    "            args, dscm, ema, dataloaders, elbo_fn, None, split='valid'\n",
    "        )\n",
    "        print(f'valid do({pa_k}) | ' + ' - '.join(f'{k}: {v:.4f}' for k, v in valid_stats.items()))\n",
    "        print(f'valid do({pa_k}) | ' + ' - '.join(f'{k}: {v:.4f}' for k, v in metrics.items()))\n",
    "args.do_pa = copy_do_pa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
